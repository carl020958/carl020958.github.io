---
title: "[PySpark] DataFrame Basics"
layout: single
date: '05/01/2022'
toc: true
toc_sticky: true
toc_label: Table of Contents
categories:
  - PYSPARK
tags:
  - PYTHON
  - PYSPARK
  - DOCKER
---

---
### PySpark DataFrame Basics
* PySpark Basic1
* DataFrame Schema
* PySpark Basic2

---

### PySpark Basic1


```python
from pyspark.sql import SparkSession
```


```python
# start spark session by applying it
spark = SparkSession.builder.appName("Basics").getOrCreate()
```


```python
# read data
df = spark.read.json("people.json")
```


```python
# show dataframe
df.show()
```

    +----+-------+
    | age|   name|
    +----+-------+
    |null|Michael|
    |  30|   Andy|
    |  19| Justin|
    +----+-------+
    



```python
# show df schema
df.printSchema()
```

    root
     |-- age: long (nullable = true)
     |-- name: string (nullable = true)
    



```python
# show df columns
df.columns
```




    ['age', 'name']




```python
# get summary of df
df.describe()
```





    DataFrame[summary: string, age: string, name: string]




```python
# # get statiscal summary of df
df.describe().show()
```

    +-------+------------------+-------+
    |summary|               age|   name|
    +-------+------------------+-------+
    |  count|                 2|      3|
    |   mean|              24.5|   null|
    | stddev|7.7781745930520225|   null|
    |    min|                19|   Andy|
    |    max|                30|Michael|
    +-------+------------------+-------+
    


---

### DataFrame Schema


```python
from pyspark.sql.types import StructField, StringType, IntegerType, StructType
```


```python
# third parameter true - no data will be null
data_schema = [StructField("age", IntegerType(),True),
               StructField("name", StringType(),True)]
```


```python
final_struc = StructType(fields=data_schema)
```


```python
df = spark.read.json('people.json', schema=final_struc)
```


```python
df.printSchema()
```

    root
     |-- age: integer (nullable = true)
     |-- name: string (nullable = true)
    


---

### PySpark Basic2

* selecting a column


```python
# select column from a df
df["age"]
```




    Column<'age'>




```python
type(df["age"])
```




    pyspark.sql.column.Column




```python
# select a column of a df as a df
df.select("age")
```




    DataFrame[age: int]




```python
df.select("age").show()
```

    +----+
    | age|
    +----+
    |null|
    |  30|
    |  19|
    +----+
    



```python
type(df.select("age"))
```




    pyspark.sql.dataframe.DataFrame




```python
# selecting multiple columns
df.select(["age","name"])
```




    DataFrame[age: int, name: string]



* selecting a row


```python
df.head(2)[0]
```




    Row(age=None, name='Michael')




```python
type(df.head(2)[0])
```




    pyspark.sql.types.Row



* mutate


```python
df.withColumn("double_age", df["age"]*2).show()
```

    +----+-------+----------+
    | age|   name|double_age|
    +----+-------+----------+
    |null|Michael|      null|
    |  30|   Andy|        60|
    |  19| Justin|        38|
    +----+-------+----------+
    


* rename


```python
df.withColumnRenamed("age", "new_age").show()
```

    +-------+-------+
    |new_age|   name|
    +-------+-------+
    |   null|Michael|
    |     30|   Andy|
    |     19| Justin|
    +-------+-------+
    


* sql in Spark


```python
# make a temporary view
df.createOrReplaceTempView("people")
```


```python
# query
results = spark.sql("SELECT * FROM people WHERE age = 30")
```


```python
results.show()
```

    +---+----+
    |age|name|
    +---+----+
    | 30|Andy|
    +---+----+
    


---
