---
title: "[Apache Airflow] SSHOperator & SparkSubmitOperator"
layout: single
date: '29/3/2022'
toc: true
toc_sticky: true
toc_label: Table of Contents
categories:
  - AIRFLOW
tags:
  - AIRFLOW
  - DOCKER
---

---
### SSHOperator & SparkSubmitOperator
* SSHOperator
* SparkSubmitOperator

---

### SSHOperator
* Spark Cluster의 Job 실행시키기 위한 SSHOperator 설치
* Spark Cluster는 [🔗 링크](https://github.com/carl020958/docker) 참고
* 추후, 비밀번호가 아닌 id_rsa를 바탕으로 ssh 연결하는 것이 더 좋은 방법으로 보임

```bash
docker container start airflow
docker container exec -it airflow bash

# install openssh-client
apt-get update
apt-get install openssh-client

# ssh connect to spark-master's root
ssh root@spark-master

# airflow virtual environment
su - airflow
cd airflow
source ./.venv/bin/activate
pip3 install apache-airflow-providers-ssh

# Connection in Airflow-Webserver
# Conn Id: ssh_spark
# Conn Type: SSH
# Host: spark-master
# Username: root
# Password: 1234
# Port:
# Extra:

# test
airflow tasks test kid_news_wordcount wordcount 2022-03-27
```
---

### SparkSubmitOperator
* 결과적으로, Airflow의 Python에는 Spark 쪽 Cluster의 Python과 다른 환경이여서 Job이 정상 작동하지 않음

```bash
docker container start airflow
docker container exec -it airflow bash

# install openjdk8 (in root)
apt-get update
apt-get install -y openjdk-8-jdk
# check
java-version

# add in user airflow's .bashrc
su - airflow
JAVA_HOME="/usr/lib/jvm/java-8-openjdk-arm64"
export PATH="$PATH:$JAVA_HOME/bin"
# don't forget
source .bashrc

# install packages for SparkSubmitOperator
pip3 install apache-airflow-providers-apache-spark

# Connection in Airflow-Webserver
# Conn Id: spark_standalone
# COnn Type: Spark
# Host: spark://spark-master
# Port: 7077
# Extra: {"queue": "root.default", "deploy_mode": "cluster", "spark_home":"/usr/bin/spark-3.1.2-bin-hadoop3.2", "spark_binary": "spark-submit", "namespace": "default"}

# test
airflow tasks test kid_news_wordcount wordcount 2022-03-12
```
---




### ref 
* [🔗 SSHOperator 참고](https://stackoverflow.com/questions/57700262/need-help-running-spark-submit-in-apache-airflow)
* [🔗 SparkSubmitOperator 참고1](https://stackoverflow.com/questions/53344285/is-there-a-way-to-submit-spark-job-on-different-server-running-master)