---
title: "[Apache Airflow] Airflow - Docker Ubuntu18.04"
layout: single
date: '8/03/2022'
toc: true
toc_sticky: true
toc_label: Table of Contents
categories:
  - AIRFLOW
tags:
  - AIRFLOW
  - DOCKER
---

---
### Airflow - Docker Ubuntu18.04
* Run Docker Ubuntu 18.04 Image
* Install Python
* Airflow Install & Configuration
* Commit & Run Docker Container
* SSHOperator
* SparkSubmitOperator

---

### Run Docker Ubuntu 18.04 Image
* Run Docker Ubuntu 18.04 Image

```bash
docker container run -it -p 8090:8080 -e LC_ALL=C.UTF-8 --name airflow ubuntu:18.04
```
---

### Install Python

```bash
# install needed packages
apt-get update -y
apt-get install -y curl
apt-get install -y build-essential
apt-get install -y zlib1g-dev
apt-get install -y libncurses5-dev
apt-get install -y libgdbm-dev
apt-get install -y libnss3-dev
apt-get install -y libssl-dev
apt-get install -y libreadline-dev
apt-get install -y libffi-dev
apt-get install -y libbz2-dev
apt-get install libsqlite3-dev

# install python
cd tmp
PYTHON_VERSION=3.8.10
curl -O https://www.python.org/ftp/python/${PYTHON_VERSION}/Python-${PYTHON_VERSION}.tgz
tar -zxvf Python-${PYTHON_VERSION}.tgz
cd Python-${PYTHON_VERSION}
./configure --enable-optimizations
make altinstall
PYTHON_VERSION2="$(python --version | cut -d " " -f 2 | cut -d "." -f 1-2)"
ln -s /usr/local/bin/${PYTHON_VERSION2} /usr/local/bin/python3
curl -O https://bootstrap.pypa.io/get-pip.py
python3 get-pip.py
rm -rf ../*
rm -rf /var/lib/apt/lists/*
```
---

### Airflow Install & Configuration

```bash
# create user
adduser airflow
# password (enter)
airflow

# create & activate virtual environment
mkdir airflow
cd airflow
python3 -m venv .venv
source ./.venv/bin/activate

# install airflow
pip3 install --upgrade pip
pip3 install --upgrade setuptools
pip3 install --upgrade distlib
AIRFLOW_HOME=$(pwd)
AIRFLOW_VERSION=2.1.4
PYTHON_VERSION="$(python --version | cut -d " " -f 2 | cut -d "." -f 1-2)"
CONSTRAINT_URL="https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt"
pip3 install "apache-airflow==${AIRFLOW_VERSION}" --constraint "${CONSTRAINT_URL}"

# add in user airflow's .bashrc
export AIRFLOW_HOME=/home/airflow/airflow

# initialize meta store meta database needed by airflow
airflow db init
```
---

### Commit & Run Docker Container
* DB를 postgres로 바꿔주는건 [🔗 링크](https://carl020958.github.io/airflow/airflow3/) 참고
* 완료 후 나머지는 local의 zsh에서 진행

```bash
docker run -p 5432:5432 --name postgres1 -e POSTGRES_PASSWORD=1234 -d -v psql_data:/var/lib/postgresql/data postgres:13

# create airflownet and include postgres in the airflownet
docker network create airflownet
docker network connect airflownet postgres1

# commit container as image
docker commit airflow carl020958/ubuntu-airflow:18.04-2.1.4-psql

# execute command in the directory 'dags' (for bind-mount)
docker container run -it -p 8090:8080 --network airflownet -v $(pwd):/home/airflow/airflow/dags -e LC_ALL=C.UTF-8 --name airflow carl020958/ubuntu-airflow:18.04-2.1.4-psql

# 시작
docker container start airflow
docker container exec -it airflow bash
```
---

### SSHOperator
* Spark Cluster의 Job 실행시키기 위한 SSHOperator 설치
* Spark Cluster는 [🔗 링크](https://github.com/carl020958/docker) 참고
* 추후, 비밀번호가 아닌 id_rsa를 바탕으로 ssh 연결하는 것이 더 좋은 방법으로 보임

```bash
docker container start airflow
docker container exec -it airflow bash

# install openssh-client
apt-get update
apt-get install openssh-client

# ssh connect to spark-master's root
ssh root@spark-master

# airflow virtual environment
su - airflow
cd airflow
source ./.venv/bin/activate
pip3 install apache-airflow-providers-ssh

# Connection in Airflow-Webserver
# Conn Id: ssh_spark
# Conn Type: SSH
# Host: spark-master
# Username: root
# Password: 1234
# Port:
# Extra:

# test
airflow tasks test kid_news_wordcount wordcount 2022-03-27
```
---

### SparkSubmitOperator
* 결과적으로, Airflow의 Python에는 Spark 쪽 Cluster의 Python과 다른 환경이여서 Job이 정상 작동하지 않음

```bash
docker container start airflow
docker container exec -it airflow bash

# install openjdk8 (in root)
apt-get update
apt-get install -y openjdk-8-jdk
# check
java-version

# add in user airflow's .bashrc
su - airflow
JAVA_HOME="/usr/lib/jvm/java-8-openjdk-arm64"
export PATH="$PATH:$JAVA_HOME/bin"
# don't forget
source .bashrc

# install packages for SparkSubmitOperator
pip3 install apache-airflow-providers-apache-spark

# Connection in Airflow-Webserver
# Conn Id: spark_standalone
# COnn Type: Spark
# Host: spark://spark-master
# Port: 7077
# Extra: {"queue": "root.default", "deploy_mode": "cluster", "spark_home":"/usr/bin/spark-3.1.2-bin-hadoop3.2", "spark_binary": "spark-submit", "namespace": "default"}

# test
airflow tasks test kid_news_wordcount wordcount 2022-03-12
```
---




### ref 
* [🔗 SSH 참고](https://kimjingo.tistory.com/71)
* [🔗 Dockerfile SSH 참고](https://stackoverflow.com/questions/27860506/openssh-server-doesnt-start-in-docker-container)
* [🔗 SSHOperator 참고](https://stackoverflow.com/questions/57700262/need-help-running-spark-submit-in-apache-airflow)
* [🔗 SparkSubmitOperator 참고1](https://stackoverflow.com/questions/53344285/is-there-a-way-to-submit-spark-job-on-different-server-running-master)