---
title: "[Hadoop] Hadoop Installation  on Local(Mac)"
layout: single
date: '22/01/2022'
toc: true
toc_sticky: true
toc_label: Table of Contents
categories:
  - HADOOP
tags:
  - HADOOP
---

---
### Hadoop Installation on Local(Mac)
* Hadoop Installation
* 환경변수 설정
* 사전 준비
* Hadoop 실행
* MapReduce 실행
* Yarn 확인
* Error 관련

---


### Hadoop Installation
* 설치환경
    * OS: macOS Big Sur 11.6
    * Hadoop: 3.3.1

```bash
brew install hadoop

# hadoop은 openjdk에 dependency가 존재하기에 hadoop 설치시 자동으로 설치됨
# 만약 기존에 사용하던 openjdk가 존재하는 경우 제거 가능
```
---

### 환경변수 설정
* directory: /usr/local/Cellar/hadoop/3.3.1/libexec/etc/hadoop

```bash
# hadoop-env.sh (맨 아래에 추가)
export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true -Djava.security.krb5.realm= -Djava.security.krb5.kdc="
export JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home"

# core-site.xml
<configuration>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/usr/local/Cellar/hadoop/hdfs/tmp</value>
        <description>A base for other temporary directories.</description>
    </property>
    <property>
        <name>fs.default.name</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>

# hdfs-site.xml
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>
</configuration>

# mapred-site.xml
<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
  <property>
    <name>mapreduce.application.classpath</name>   
    <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>
  </property>
</configuration>

# yarn-site.xml
<configuration>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
  <property>
    <name>yarn.nodemanager.env-whitelist</name>
    <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
  </property>
</configuration>
```
---

### 사전 준비
```bash
# 아래 명령어 입력
ssh localhost
# 만약 접속시간이 안뜨고 Connection refused 라고 뜨면, 아래 명령어를 입력
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 0600 ~/.ssh/authorized_keys

# 디렉토리 변경
cd /usr/local/Cellar/hadoop/3.3.1/libexec/bin

# HDFS 포맷
hdfs namenode -format
# 포맷시, /usr/local/Cellar/hadoop에 hdfs 생성됨

# alias 등록
alias hstart="/usr/local/Cellar/hadoop/3.3.1/libexec/sbin/start-all.sh"
alias hstop="/usr/local/Cellar/hadoop/3.3.1/libexec/sbin/stop-all.sh"
```
---

### Hadoop 실행
* Cluster status : http://localhost:8088
* HDFS status : http://localhost:9870 
* Secondary NameNode status : http://localhost:9868

```bash
# 하둡 시작
hstart
# 만약 "localhost: ssh: connect to host localhost port 22: Connection refused" 발생시
# System Preferences - Sharing - Remote Login 접속후
# 1) Remote Login을 on으로 수정
# 2) Allow full disk access for remote users 클릭

# 확인
jps 
# NodeManager, NameNode, Jps, DataNode, ResourceManager, SecondaryNameNode가 정상적으로 나오는지 확인

# 종료
hstop
```
---

### MapReduce 실행
* local에 wFile01.txt, wFile02.txt 준비 후 해당 디렉토리에서 진행

```bash
#-------- 
# 파일 주입
#--------

# 디렉토리 생성
hadoop fs -mkdir /input01

# 디렉토리 확인
hadoop fs -ls /

# 파일 옮기기
hadoop fs -put ./wFile01.txt /input01
hadoop fs -put ./wFile02.txt /input01

# 확인
hadoop fs -ls /input01
hadoop fs -cat /input01/wFile01.txt
hadoop fs -cat /input01/wFile02.txt

# jar 파일 안의 wordcount 클래스를 이용해, input01 안의 파일에 mapreduce에 적용 후, output01로
hadoop jar /usr/local/Cellar/hadoop/3.3.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar wordcount /input01 /output01

# 확인
hadoop fs -cat /output01/
```
---

### Yarn 확인

```bash
yarn jar /opt/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar pi 16 100000
```
---

### Error
* start-all.sh로 하둡 실행 시, secondarynodemanager과 관련해 operation timed out 에러 발생
* 원인: 집 공유기가 SK인데, SK에서는 22번 포트를 막아 놓은 경우가 있다고 함
* 임시적인 해결: 핸드폰 핫스팟으로 진행 시 정상 작동 확인

---

### ref
* [🔗 공식문서](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html)
* [🔗 참고1](https://key4920.github.io/p/mac-os에-하둡hadoop-설치/)
* [🔗 참고2](https://stackoverflow.com/questions/51808588/run-hadoop-in-the-mac-os)